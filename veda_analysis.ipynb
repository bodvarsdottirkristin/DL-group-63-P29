{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade7a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "import importlib\n",
    "import utm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import veda.ais_to_parquet\n",
    "from veda.ais_to_parquet import fn\n",
    "import veda.trajectory_segmentation\n",
    "from veda.trajectory_segmentation import calculate_max_radius, segment_by_stationary_periods\n",
    "import veda.interpolation\n",
    "from veda.interpolation import regularize_trajectory, regularize_all_trajectories\n",
    "import veda.coord_to_utm\n",
    "from veda.coord_to_utm import to_utm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d1159e",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "I adjusted the `ais-to-parquet.py` script to also extract ship type. We may consider also including:\n",
    "- ROT\n",
    "- Heading\n",
    "- Destination\n",
    "- ETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca293d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn('data/ais_data/aisdk-2025-11-01.csv', 'data/ais_data/aisdk-2025-11-01.parquet')\n",
    "# fn('data/ais_data/aisdk-2025-11-02.csv', 'data/ais_data/aisdk-2025-11-02.parquet')\n",
    "# fn('data/ais_data/aisdk-2025-11-03.csv', 'data/ais_data/aisdk-2025-11-03.parquet')\n",
    "# fn('data/ais_data/aisdk-2025-11-04.csv', 'data/ais_data/aisdk-2025-11-04.parquet')\n",
    "# fn('data/ais_data/aisdk-2025-11-05.csv', 'data/ais_data/aisdk-2025-11-05.parquet')\n",
    "# fn('data/ais_data/aisdk-2025-11-06.csv', 'data/ais_data/aisdk-2025-11-06.parquet')\n",
    "# fn('data/ais_data/aisdk-2025-11-07.csv', 'data/ais_data/aisdk-2025-11-07.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab5a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all parquet files into a single df\n",
    "parquet_files = [\n",
    "    'data/ais_data/aisdk-2025-11-01.parquet',\n",
    "    'data/ais_data/aisdk-2025-11-02.parquet'#,\n",
    "    # 'data/ais_data/aisdk-2025-11-03.parquet',\n",
    "    # 'data/ais_data/aisdk-2025-11-04.parquet',\n",
    "    # 'data/ais_data/aisdk-2025-11-05.parquet',\n",
    "    # 'data/ais_data/aisdk-2025-11-06.parquet',\n",
    "    # 'data/ais_data/aisdk-2025-11-07.parquet'\n",
    "]\n",
    "dfs = [pd.read_parquet(file) for file in parquet_files]\n",
    "df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37041e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Segment' column (since we define our own segments later)\n",
    "df = df.drop(columns=['Segment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f55266d",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4d0a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total vessels: \", df['MMSI'].unique().shape[0])\n",
    "\n",
    "cdf = df[df['Ship type'] == 'Cargo'].drop(columns=['Ship type'])\n",
    "print(\"Cargo vessels: \", cdf['MMSI'].unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2045edd",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "Here I check to see where we have missing values. Since it seems that there are only a few vessels missing SOG and COG values, we could either calculate them manually based on lat/long and time, or we could just drop the affected vessels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64038795",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d7146",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_cog = cdf[cdf['COG'].isnull()]['MMSI'].unique()\n",
    "m_sog = cdf[cdf['SOG'].isnull()]['MMSI'].unique()\n",
    "\n",
    "m_missing = set(m_cog).union(set(m_sog))\n",
    "print(\"Number of ships with missing COG or SOG: \", len(m_missing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4fa44c",
   "metadata": {},
   "source": [
    "Here we need to decide whether we want to drop the ships with missing COG/SOG values, or whether we want to estimate them using positional and temporal data. \n",
    "\n",
    "**For now I will just drop them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b3c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_clean = cdf[~cdf['MMSI'].isin(m_missing)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5631fd",
   "metadata": {},
   "source": [
    "## Trajectory Segmentation Based on Stationary Periods\n",
    "\n",
    "Split trajectories whenever the ship is stationary for more than 30 minutes. A ship is considered stationary if:\n",
    "- SOG < 1 knot (already in m/s after conversion), OR\n",
    "- Position variance < 50m over the stationary period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b9578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(veda.trajectory_segmentation)\n",
    "from veda.trajectory_segmentation import segment_by_stationary_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9018ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment trajectories\n",
    "cdf_segmented = segment_by_stationary_periods(\n",
    "    cdf_clean,\n",
    "    sog_threshold=0.5,        # 1 knot in m/s\n",
    "    position_threshold=50,    # 50 meters\n",
    "    time_threshold=30         # 30 minutes\n",
    ")\n",
    "\n",
    "print(f\"Total vessels: {cdf_segmented['MMSI'].nunique()}\")\n",
    "print(f\"Total trajectories: {cdf_segmented['Trajectory'].nunique()}\")\n",
    "print(f\"Average trajectories per vessel: {cdf_segmented['Trajectory'].nunique() / cdf_segmented['MMSI'].nunique():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c725650",
   "metadata": {},
   "outputs": [],
   "source": [
    "ships_with_multiple_trajectories = cdf_segmented.groupby('MMSI')['Trajectory'].nunique()\n",
    "multi_traj_ships = ships_with_multiple_trajectories[ships_with_multiple_trajectories > 1]\n",
    "\n",
    "if len(multi_traj_ships) > 0:\n",
    "    sample_mmsi = multi_traj_ships.index[3]\n",
    "    sample_ship_df = cdf_segmented[cdf_segmented['MMSI'] == sample_mmsi].sort_values('Timestamp')\n",
    "    \n",
    "    print(f\"Visualizing MMSI {sample_mmsi} with {sample_ship_df['Trajectory'].nunique()} trajectories\")\n",
    "    \n",
    "    # Create color map for trajectories\n",
    "    fig = px.line_map(\n",
    "        sample_ship_df,\n",
    "        lat=\"Latitude\",\n",
    "        lon=\"Longitude\",\n",
    "        color=\"Trajectory\",\n",
    "        hover_data=[\"Timestamp\", \"SOG\"],\n",
    "        zoom=5,\n",
    "        title=f\"Segmented Trajectories for MMSI {sample_mmsi}\"\n",
    "    )\n",
    "    fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No ships with multiple trajectories found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbd1595",
   "metadata": {},
   "source": [
    "## Time Series Regularization\n",
    "\n",
    "Resample trajectories to regular time intervals using linear interpolation. This ensures consistent sampling frequency for RNN training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b5ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(veda.interpolation)\n",
    "from veda.interpolation import regularize_all_trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERVAL_MINUTES = 5 # (I picked 5 arbitrarily)\n",
    "\n",
    "cdf_regular = regularize_all_trajectories(cdf_segmented, interval_minutes=INTERVAL_MINUTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a674db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_traj = cdf_regular[cdf_regular['Trajectory'] == 50].sort_values('Timestamp')\n",
    "time_diffs = sample_traj['Timestamp'].diff().dt.total_seconds() / 60\n",
    "print(f\"\\nMean interval: {time_diffs.mean():.2f} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b827ab5",
   "metadata": {},
   "source": [
    "## Convert to UTM Coordinates\n",
    "\n",
    "Convert latitude/longitude to UTM (as they did in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe05d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(veda.coord_to_utm)\n",
    "from veda.coord_to_utm import to_utm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778713f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_utm = to_utm(cdf_regular)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d23c91",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Features we'll definitely use:\n",
    "- UTM coordinates\n",
    "- SOG\n",
    "- COG (decomposed into east/north velocities)\n",
    "\n",
    "Others we could consider:\n",
    "- Spatial context\n",
    "    - Acceleration\n",
    "    - Heading\n",
    "    - ROT\n",
    "    - Distance from coast/port/shipping lanes\n",
    "    - Water depth\n",
    "- Temporal context\n",
    "    - Tidal data\n",
    "    - Time of day\n",
    "    - Day of week\n",
    "    - Season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54100f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose COG into its vector components\n",
    "cog_radians = np.radians(cdf_utm['COG'])                 # convert to radians\n",
    "cdf_utm['v_east'] = cdf_utm['SOG'] * np.sin(cog_radians)      # eastward component\n",
    "cdf_utm['v_north'] = cdf_utm['SOG'] * np.cos(cog_radians)     # northward component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4823eec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_utm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835f5037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into different trajectories, keeping only relevant features\n",
    "fdf = cdf_utm[['Trajectory', 'Timestamp', 'UTM_x', 'UTM_y', 'SOG', 'v_east', 'v_north']].copy()\n",
    "\n",
    "trajectories = []\n",
    "    \n",
    "for traj_id in fdf['Trajectory'].unique():\n",
    "    traj_data = fdf[fdf['Trajectory'] == traj_id].sort_values('Timestamp')\n",
    "    features = traj_data[['UTM_x', 'UTM_y', 'SOG', 'v_east', 'v_north']].values\n",
    "    trajectories.append(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b12392d",
   "metadata": {},
   "source": [
    "## Train-Test-Val Split\n",
    "\n",
    "I just did a simple random sample with 70% train, 15% test, 15% val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90493ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, temp = train_test_split(trajectories, test_size=0.3, random_state=42)\n",
    "val, test = train_test_split(temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9ad02e",
   "metadata": {},
   "source": [
    "## Normalisation\n",
    "\n",
    "Options to consider:\n",
    "- min-max scaling\n",
    "- z-score\n",
    "- feature-specific scaling\n",
    "    - I think min-max is appropriate for the simple case in which we just look at UTM, SOG, and COG, but that might change if we add more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc330b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit min-max scaler to training set\n",
    "train_stacked = np.vstack(train)\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaler.fit(train_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c031282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply min-max scaling to each set\n",
    "train_s = [scaler.transform(traj) for traj in train]\n",
    "val_s = [scaler.transform(traj) for traj in val]\n",
    "test_s = [scaler.transform(traj) for traj in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce8852f",
   "metadata": {},
   "source": [
    "### Set up dataset/dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28618a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Trajectory Dataset\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, trajectories):\n",
    "        self.trajectories = trajectories\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        traj = torch.FloatTensor(self.trajectories[idx])\n",
    "        return traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0522e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trajectories(batch):\n",
    "    lengths = torch.tensor([len(traj) for traj in batch])\n",
    "    padded = pad_sequence(batch, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    lengths, perm_idx = lengths.sort(descending=True)\n",
    "    padded = padded[perm_idx]\n",
    "\n",
    "    return padded, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354cee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrajectoryDataset(train_s)\n",
    "val_dataset = TrajectoryDataset(val_s)\n",
    "test_dataset = TrajectoryDataset(test_s)\n",
    "\n",
    "batch_size = 32 # we need to pick one, this is arbitrary\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=pad_trajectories)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_trajectories)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=pad_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acaeb40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see what's goin' on\n",
    "for batch, lengths in train_loader:\n",
    "    print(f\"Batch shape: {batch.shape}\")  # (batch_size, max_length, 5)\n",
    "    print(f\"Lengths: {lengths}\")  # (batch_size,)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eedd047",
   "metadata": {},
   "source": [
    "# Train the VRAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2680739b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
