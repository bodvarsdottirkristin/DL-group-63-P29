{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91285ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from classification_rnn import ClassificationRNN, DEVICE\n",
    "from seed import set_seed\n",
    "from config import flatten_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2077a49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"../../data/aisdk/processed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8f50a",
   "metadata": {},
   "source": [
    "# 1 - Create the windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35f947ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ADD A CLASSIFIER HERE!!\n",
    "def _get_cluster_id_for_segment(traj_id):\n",
    "    \"\"\"\n",
    "    Assign cluster to a given segment\n",
    "    PLACEHOLDER FOR NOW\n",
    "    \"\"\"\n",
    "    return random.randint(0, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af907f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_past_future_windows_np(\n",
    "    past_len=30,\n",
    "    future_len=30,\n",
    "    step=1,\n",
    "    input_path=\"data/aisdk/processed/train_trajectories.npz\",\n",
    "    output_path=\"data/aisdk/processed/windows/train_trajectories.npz\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Load standardized, sorted trajectory data from an npz file,\n",
    "    create past/future sliding windows, and save everything as NumPy arrays\n",
    "    in a single .npz file:\n",
    "\n",
    "        - past:    (N, past_len, num_features)\n",
    "        - future:  (N, future_len, num_features)\n",
    "        - traj_id: (N,) - original trajectory ID for each window\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading trajectories from {input_path} ...\")\n",
    "    data = np.load(input_path, allow_pickle=True)\n",
    "    trajs = data[\"trajectories\"]  # (N,) array of variable-length arrays (T_i, F)\n",
    "    traj_ids = data[\"traj_ids\"]   # (N,)\n",
    "    feature_cols = data[\"feature_cols\"]  # feature names\n",
    "    num_traj = len(trajs)\n",
    "    print(f\"  → Found {num_traj} trajectories\")\n",
    "    print(f\"  → Features: {feature_cols}\")\n",
    "\n",
    "    total_len = past_len + future_len\n",
    "\n",
    "    past_list = []\n",
    "    future_list = []\n",
    "    traj_list = []\n",
    "    cluster_list = []\n",
    "\n",
    "    for i, traj in enumerate(trajs):\n",
    "        # traj: (T_i, F)\n",
    "        T = traj.shape[0]\n",
    "        if T < total_len:\n",
    "            continue  # too short for one window\n",
    "\n",
    "        # number of windows for this trajectory (with stride `step`)\n",
    "        num_windows = (T - total_len) // step + 1\n",
    "\n",
    "        # assign a cluster\n",
    "        cid = _get_cluster_id_for_segment(traj_ids[i])\n",
    "\n",
    "        for w in range(num_windows):\n",
    "            start = w * step\n",
    "            mid   = start + past_len\n",
    "            end   = mid + future_len\n",
    "\n",
    "            past_window = traj[start:mid]   # (past_len, F)\n",
    "            future_window = traj[mid:end]   # (future_len, F)\n",
    "\n",
    "            past_list.append(past_window)\n",
    "            future_list.append(future_window)\n",
    "            traj_list.append(traj_ids[i])\n",
    "            cluster_list.append(cid)\n",
    "\n",
    "        if (i + 1) % 500 == 0:\n",
    "            print(f\"  → Processed {i+1}/{num_traj} trajectories, {len(past_list)} windows so far\")\n",
    "\n",
    "    if not past_list:\n",
    "        raise RuntimeError(\"No windows generated. \"\n",
    "                           \"Check past_len, future_len, and trajectory lengths.\")\n",
    "\n",
    "    print(\"Stacking windows into numpy arrays...\")\n",
    "    past = np.stack(past_list)     # (N, past_len, F)\n",
    "    future = np.stack(future_list) # (N, future_len, F)\n",
    "    traj_id = np.array(traj_list)  # (N,)\n",
    "    cluster = np.array(cluster_list)\n",
    "\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    print(f\"Saving windows to {output_path} ...\")\n",
    "    np.savez_compressed(\n",
    "        output_path,\n",
    "        past=past,\n",
    "        future=future,\n",
    "        cluster=cluster,\n",
    "        traj_id=traj_id,\n",
    "        feature_cols=feature_cols,\n",
    "        past_len=past_len,\n",
    "        future_len=future_len,\n",
    "        step=step,\n",
    "    )\n",
    "\n",
    "    print(\"\\nDONE!\")\n",
    "    print(f\"  → Total windows: {past.shape[0]:,}\")\n",
    "    print(f\"  → Shape - past: {past.shape}, future: {future.shape}\")\n",
    "    print(f\"  → Output saved to: {output_path}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    unique_trajs = np.unique(traj_id)\n",
    "    print(f\"  → Windows span {len(unique_trajs)} unique trajectories\")\n",
    "    print(f\"  → Avg windows per trajectory: {len(past_list) / len(unique_trajs):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73a51846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trajectories from ../../data/aisdk/processed/train_trajectories.npz ...\n",
      "  → Found 359 trajectories\n",
      "  → Features: ['UTM_x' 'UTM_y' 'SOG' 'v_east' 'v_north']\n",
      "Stacking windows into numpy arrays...\n",
      "Saving windows to ../../data/aisdk/processed/windows/train_trajectories.npz ...\n",
      "\n",
      "DONE!\n",
      "  → Total windows: 171,461\n",
      "  → Shape - past: (171461, 30, 5), future: (171461, 30, 5)\n",
      "  → Output saved to: ../../data/aisdk/processed/windows/train_trajectories.npz\n",
      "  → Windows span 315 unique trajectories\n",
      "  → Avg windows per trajectory: 544.3\n",
      "Loading trajectories from ../../data/aisdk/processed/val_trajectories.npz ...\n",
      "  → Found 77 trajectories\n",
      "  → Features: ['UTM_x' 'UTM_y' 'SOG' 'v_east' 'v_north']\n",
      "Stacking windows into numpy arrays...\n",
      "Saving windows to ../../data/aisdk/processed/windows/val_trajectories.npz ...\n",
      "\n",
      "DONE!\n",
      "  → Total windows: 36,719\n",
      "  → Shape - past: (36719, 30, 5), future: (36719, 30, 5)\n",
      "  → Output saved to: ../../data/aisdk/processed/windows/val_trajectories.npz\n",
      "  → Windows span 66 unique trajectories\n",
      "  → Avg windows per trajectory: 556.3\n",
      "Loading trajectories from ../../data/aisdk/processed/test_trajectories.npz ...\n",
      "  → Found 78 trajectories\n",
      "  → Features: ['UTM_x' 'UTM_y' 'SOG' 'v_east' 'v_north']\n",
      "Stacking windows into numpy arrays...\n",
      "Saving windows to ../../data/aisdk/processed/windows/test_trajectories.npz ...\n",
      "\n",
      "DONE!\n",
      "  → Total windows: 33,842\n",
      "  → Shape - past: (33842, 30, 5), future: (33842, 30, 5)\n",
      "  → Output saved to: ../../data/aisdk/processed/windows/test_trajectories.npz\n",
      "  → Windows span 67 unique trajectories\n",
      "  → Avg windows per trajectory: 505.1\n"
     ]
    }
   ],
   "source": [
    "make_past_future_windows_np(\n",
    "    input_path=os.path.join(BASE, \"train_trajectories.npz\"),\n",
    "    output_path=os.path.join(BASE, \"windows/train_trajectories.npz\"),\n",
    ")\n",
    "\n",
    "make_past_future_windows_np(\n",
    "    input_path  = os.path.join(BASE, \"val_trajectories.npz\"),\n",
    "    output_path = os.path.join(BASE, \"windows/val_trajectories.npz\"),\n",
    ")\n",
    "\n",
    "make_past_future_windows_np(\n",
    "    input_path  = os.path.join(BASE, \"test_trajectories.npz\"),\n",
    "    output_path = os.path.join(BASE, \"windows/test_trajectories.npz\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0309bcdd",
   "metadata": {},
   "source": [
    "# 1 - Fetch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e067cc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_traj = np.load(os.path.join(BASE, \"windows/train_trajectories.npz\"))\n",
    "val_traj = np.load(os.path.join(BASE, \"windows/val_trajectories.npz\"))\n",
    "test_traj = np.load(os.path.join(BASE, \"windows/test_trajectories.npz\"))\n",
    "\n",
    "X_train, X_val = train_traj[\"past\"], val_traj[\"past\"]\n",
    "y_train, y_val = train_traj[\"cluster\"], val_traj[\"cluster\"]\n",
    "\n",
    "# %% Convert to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val_t   = torch.tensor(X_val,   dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val_t   = torch.tensor(y_val,   dtype=torch.long)\n",
    "\n",
    "# Create data loaders\n",
    "print(f\"\\nTensor shapes:\")\n",
    "print(f\"  X_train_t: {X_train_t.shape}\")\n",
    "print(f\"  X_val_t:   {X_val_t.shape}\")\n",
    "print(f\"  y_train_t: {y_train_t.shape}\")\n",
    "print(f\"  y_val_t:   {y_val_t.shape}\")\n",
    "\n",
    "# %% Create data loaders\n",
    "def make_loaders(batch_size):\n",
    "    train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588d060b",
   "metadata": {},
   "source": [
    "# 2 - Train\n",
    "## 2.1 - Define functions for one run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42390d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_one_run(cfg, train_loader, val_loader):\n",
    "    \n",
    "    device = cfg[\"device\"]\n",
    "\n",
    "    model = ClassificationRNN(\n",
    "        input_size=cfg[\"input_size\"],\n",
    "        hidden_size=cfg[\"hidden_size\"],\n",
    "        num_classes=cfg[\"num_classes\"], \n",
    "    ).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(\n",
    "        model.parameters(), \n",
    "        lr=cfg[\"lr\"], \n",
    "        weight_decay=cfg[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    crit = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    best_val_acc  = 0.0\n",
    "\n",
    "    # %% Training loop\n",
    "    for epoch in range(cfg[\"epochs\"]):\n",
    "    \n",
    "        # ------ Training phase ------\n",
    "        model.train()\n",
    "        train_loss_total = 0.0\n",
    "        train_correct = 0\n",
    "        train_samples = 0\n",
    "        \n",
    "        for xb, yb in tqdm(train_loader, desc=f\"Epoch {epoch}/{cfg[\"epochs\"]} [Train]\"):\n",
    "            xb = xb.to(device)  # (B, seq_len, num_features)\n",
    "            yb = yb.to(device)  # (B,)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)  # (B, num_classes)\n",
    "            loss = crit(logits, yb)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\"grad_clip\"])\n",
    "            opt.step()\n",
    "\n",
    "            train_loss_total += loss.item() #* xb.size(0)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            preds = logits.argmax(dim=1)  # (B,)\n",
    "            train_correct += (preds == yb).sum().item()\n",
    "            train_samples += yb.size(0)\n",
    "\n",
    "        train_loss = train_loss_total / len(train_loader)\n",
    "        train_acc = train_correct / train_samples if train_samples > 0 else 0.0\n",
    "\n",
    "        # ------ Validation ------\n",
    "        model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in tqdm(val_loader, desc=f\"Epoch {epoch}/{cfg[\"epochs\"]} [Val]  \"):\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                logits = model(xb)\n",
    "                loss = crit(logits, yb)\n",
    "                val_loss_total += loss.item() #*xb.size(0)\n",
    "\n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == yb).sum().item()\n",
    "                val_samples += yb.size(0)\n",
    "\n",
    "        val_loss = val_loss_total / len(val_loader)\n",
    "        val_acc = val_correct / val_samples if val_samples > 0 else 0.0\n",
    "        \n",
    "        # Update best validation loss\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            print(f\"  → New best validation loss: {best_val_loss:.4f}\")\n",
    "        \n",
    "        return best_val_loss, best_val_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f09577",
   "metadata": {},
   "source": [
    "## 2.2 - Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key metrics\n",
    "input_size = X_train.shape[-1]\n",
    "n_classes = int(y_train.max() + 1)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search_classification(\n",
    "    device=device,\n",
    "    input_size=input_size,\n",
    "    num_classes=n_classes,\n",
    "    search_type='grid',\n",
    "    save_dir='../../checkpoints/hyperparameter_results_classification'\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"HYPERPARAMETER TUNING FOR CLASSIFICATION RNN\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # -------- 1) Define search space --------\n",
    "    if search_type == 'grid':\n",
    "        param_grid = {\n",
    "            # Tune these:\n",
    "            'hidden_size'   : [64, 128],\n",
    "            'num_layers'    : [1, 2],\n",
    "            'dropout'       : [0.0, 0.3],\n",
    "            'lr'            : [1e-4, 3e-4, 1e-3],\n",
    "            'weight_decay'  : [0.0, 1e-4],\n",
    "            \n",
    "            # Fixed:\n",
    "            'batch_size'    : [256],\n",
    "            'epochs'        : [20],\n",
    "            #'patience'     : [5],\n",
    "            #'min_delta'    : [1e-4],\n",
    "            'grad_clip'     : [1.0],\n",
    "        }\n",
    "\n",
    "        # total combos: 2 * 2 * 2 * 3 = 24\n",
    "        \n",
    "    elif search_type == 'quick':\n",
    "        # Smaller search – good for smoke-testing\n",
    "        param_grid = {\n",
    "            'hidden_size'   : [64, 128],\n",
    "            'num_layers'    : [1],\n",
    "            'dropout'       : [0.0, 0.3],\n",
    "            'lr'            : [3e-4],\n",
    "            'weight_decay'  : [0.0, 1e-4],\n",
    "\n",
    "            \n",
    "            'batch_size'    : [256],\n",
    "            'epochs'        : [15],\n",
    "            #'patience'     : [3],\n",
    "            #'min_delta'    : [1e-4],\n",
    "            'grad_clip'     : [1.0],\n",
    "        }\n",
    "        # total combos: 2 * 1 * 2 * 1 = 4\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown search_type: {search_type}\")\n",
    "    \n",
    "    # Generate combinations\n",
    "    keys = list(param_grid.keys())\n",
    "    values = list(param_grid.values())\n",
    "    combinations = list(itertools.product(*values))\n",
    "    param_combinations = [dict(zip(keys, combo)) for combo in combinations]\n",
    "    \n",
    "    print(f\"{search_type.capitalize()} Search: {len(param_combinations)} combinations\")\n",
    "    # super rough estimate: 5 min / combo like your friend\n",
    "    print(f\"Estimated time: {len(param_combinations) * 5} minutes\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # -------- 2) Tracking --------\n",
    "    results = []\n",
    "    best_score = float('inf')   # here: best_val_loss\n",
    "    best_params = None\n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # -------- 3) Main search loop --------\n",
    "    for idx, params in enumerate(param_combinations):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Trial {idx+1}/{len(param_combinations)}\")\n",
    "        elapsed_min = (datetime.now() - start_time).total_seconds() / 60\n",
    "        print(f\"Time elapsed: {elapsed_min:.1f} min\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        print(\"Testing config:\")\n",
    "        print(f\"  hidden_size   = {params['hidden_size']}\")\n",
    "        print(f\"  num_layers    = {params['num_layers']}\")\n",
    "        print(f\"  dropout       = {params['dropout']}\")\n",
    "        print(f\"  lr            = {params['lr']}\")\n",
    "        print(f\"  weight_decay  = {params['weight_decay']}\")\n",
    "        print(f\"  batch_size    = {params['batch_size']}\")\n",
    "        print(f\"  epochs        = {params['epochs']}\")\n",
    "        print()\n",
    "        \n",
    "        try:\n",
    "            # ---- Build config dict for your existing _train_one_run ----\n",
    "            cfg = {\n",
    "                \"device\": device,\n",
    "                \"input_size\": input_size,\n",
    "                \"num_classes\": num_classes,\n",
    "                \n",
    "                # training hyperparams\n",
    "                \"hidden_size\": params[\"hidden_size\"],\n",
    "                \"num_layers\": params[\"num_layers\"],\n",
    "                \"dropout\"   : params[\"dropout\"],\n",
    "                \"lr\"        : params[\"lr\"],\n",
    "                \"weight_decay\": params[\"weight_decay\"],\n",
    "                \"batch_size\": params[\"batch_size\"],\n",
    "                \"epochs\"    : params[\"epochs\"],\n",
    "                #\"patience\": params[\"patience\"],\n",
    "                #\"min_delta\": params[\"min_delta\"],\n",
    "                \"grad_clip\": params[\"grad_clip\"],\n",
    "            }\n",
    "            \n",
    "            train_loader, val_loader = make_loaders(cfg[\"batch_size\"])\n",
    "\n",
    "            model, train_losses, val_losses, val_accs = _train_one_run(cfg, train_loader, val_loader)\n",
    "            \n",
    "            # ---- Extract metrics ----\n",
    "            val_losses = list(val_losses)\n",
    "            best_val_loss = float(min(val_losses))\n",
    "            best_epoch = int(val_losses.index(best_val_loss) + 1)\n",
    "            \n",
    "            if val_accs is not None:\n",
    "                val_accs = list(val_accs)\n",
    "                best_val_acc = float(val_accs[best_epoch - 1])\n",
    "            else:\n",
    "                best_val_acc = float(\"nan\")\n",
    "            \n",
    "            final_train_loss = float(train_losses[best_epoch - 1])\n",
    "            overfit_ratio = best_val_loss / max(final_train_loss, 1e-6)\n",
    "            \n",
    "            score = best_val_loss  # primary metric: lower is better\n",
    "            \n",
    "            result = {\n",
    "                **params,\n",
    "                'best_val_loss': best_val_loss,\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'best_epoch': best_epoch,\n",
    "                'final_train_loss': final_train_loss,\n",
    "                'overfit_ratio': overfit_ratio,\n",
    "                'score': score,\n",
    "                'trial': idx + 1,\n",
    "            }\n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"\\nResults:\")\n",
    "            print(f\"  Best Val Loss: {best_val_loss:.4f} (epoch {best_epoch})\")\n",
    "            if not np.isnan(best_val_acc):\n",
    "                print(f\"  Best Val Acc : {best_val_acc:.4f}\")\n",
    "            print(f\"  Train Loss @best: {final_train_loss:.4f}\")\n",
    "            print(f\"  Overfit ratio: {overfit_ratio:.2f}\")\n",
    "            print(f\"  Score (val_loss): {score:.4f}\")\n",
    "            \n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_params = params.copy()\n",
    "                print(f\"  ⭐ NEW BEST CONFIG FOUND!\")\n",
    "                \n",
    "                \n",
    "                # Save best model\n",
    "                torch.save(model.state_dict(), os.path.join(save_dir, \"best_model.pth\"))\n",
    "            \n",
    "            # Clean up\n",
    "            del model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ FAILED: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            result = {**params, 'error': str(e), 'trial': idx + 1}\n",
    "            results.append(result)\n",
    "    \n",
    "    # -------- 4) Save & summarize --------\n",
    "    if len(results) == 0:\n",
    "        print(\"\\n❌ No trials completed!\")\n",
    "        return None, None\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # Sort by score (val loss)\n",
    "    if 'score' in df.columns:\n",
    "        df = df.sort_values('score', na_position='last')\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    csv_path = os.path.join(save_dir, f\"results_{timestamp}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    if best_params is None:\n",
    "        print(\"\\n❌ All trials failed! Check error messages above.\")\n",
    "        return df, None\n",
    "    \n",
    "    json_path = os.path.join(save_dir, f\"best_params_{timestamp}.json\")\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "    \n",
    "    total_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"HYPERPARAMETER SEARCH COMPLETE\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total time: {total_time:.1f} minutes\")\n",
    "    successful = [r for r in results if 'error' not in r]\n",
    "    print(f\"Successful trials: {len(successful)}/{len(results)}\")\n",
    "    \n",
    "    print(f\"\\nBest parameters (score={best_score:.4f}):\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    \n",
    "    print(f\"\\nResults saved to: {csv_path}\")\n",
    "    print(f\"Best params saved to: {json_path}\")\n",
    "    print(f\"Best model saved to: {os.path.join(save_dir, 'best_model.pth')}\")\n",
    "    \n",
    "    # Show top 5 successful trials\n",
    "    successful_df = df[df['score'].notna()]\n",
    "    top_cols = [\n",
    "        'hidden_size', 'num_layers', 'dropout', 'learning_rate',\n",
    "        'batch_size', 'best_val_loss', 'best_val_acc', 'score'\n",
    "    ]\n",
    "    print(\"\\nTop 5 configurations:\")\n",
    "    print(successful_df[top_cols].head())\n",
    "    \n",
    "    return df, best_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b993f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPERPARAMETER TUNING FOR CLASSIFICATION RNN\n",
      "======================================================================\n",
      "Grid Search: 48 combinations\n",
      "Estimated time: 240 minutes\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Trial 1/48\n",
      "Time elapsed: 0.0 min\n",
      "======================================================================\n",
      "Testing config:\n",
      "  hidden_size   = 64\n",
      "  num_layers    = 1\n",
      "  dropout       = 0.0\n",
      "  lr            = 0.0001\n",
      "  weight_decay  = 0.0\n",
      "  batch_size    = 256\n",
      "  epochs        = 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/20 [Train]:  44%|████▍     | 295/669 [01:03<01:20,  4.66it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_cls, best_params_cls = \u001b[43mhyperparameter_search_classification\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43msearch_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mgrid\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhyperparameter_results_classification\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mhyperparameter_search_classification\u001b[39m\u001b[34m(device, input_size, num_classes, search_type, save_dir)\u001b[39m\n\u001b[32m    105\u001b[39m cfg = {\n\u001b[32m    106\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m: device,\n\u001b[32m    107\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33minput_size\u001b[39m\u001b[33m\"\u001b[39m: input_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m    120\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mgrad_clip\u001b[39m\u001b[33m\"\u001b[39m: params[\u001b[33m\"\u001b[39m\u001b[33mgrad_clip\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    121\u001b[39m }\n\u001b[32m    123\u001b[39m train_loader, val_loader = make_loaders(cfg[\u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m model, train_losses, val_losses, val_accs = \u001b[43m_train_one_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m# ---- Extract metrics ----\u001b[39;00m\n\u001b[32m    128\u001b[39m val_losses = \u001b[38;5;28mlist\u001b[39m(val_losses)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36m_train_one_run\u001b[39m\u001b[34m(cfg, train_loader, val_loader)\u001b[39m\n\u001b[32m     33\u001b[39m yb = yb.to(device)  \u001b[38;5;66;03m# (B,)\u001b[39;00m\n\u001b[32m     35\u001b[39m opt.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (B, num_classes)\u001b[39;00m\n\u001b[32m     37\u001b[39m loss = crit(logits, yb)\n\u001b[32m     38\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/src/models/classification_rnn.py:60\u001b[39m, in \u001b[36mClassificationRNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor) -> torch.Tensor:\n\u001b[32m     57\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m    x: (B, T, input_size) numeric features only\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     out, h_n = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bidirectional:\n\u001b[32m     63\u001b[39m         \u001b[38;5;66;03m# last layer forward/backward\u001b[39;00m\n\u001b[32m     64\u001b[39m         h_forward = h_n[-\u001b[32m2\u001b[39m, :, :]   \u001b[38;5;66;03m# (B, H)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/torch/nn/modules/rnn.py:1394\u001b[39m, in \u001b[36mGRU.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1392\u001b[39m \u001b[38;5;28mself\u001b[39m.check_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1394\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1395\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1396\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1397\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1398\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1399\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1402\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1406\u001b[39m     result = _VF.gru(\n\u001b[32m   1407\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1408\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1415\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1416\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run the training\n",
    "df_cls, best_params_cls = hyperparameter_search_classification(\n",
    "    device=device,\n",
    "    input_size=input_size,\n",
    "    num_classes=n_classes,\n",
    "    search_type='grid',\n",
    "    save_dir='../../checkpoints/hyperparameter_results_classification' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c708db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total combinations: 96\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to True."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grid_run_0</strong> at: <a href='https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid/runs/8xsb1pxk' target=\"_blank\">https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid/runs/8xsb1pxk</a><br> View project at: <a href='https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid' target=\"_blank\">https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251201_155911-8xsb1pxk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/helgamariamagnusdottir/Documents/dtu/deep_learning/DL-group-63-P29/src/models/wandb/run-20251201_160006-76lnj2x4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid/runs/76lnj2x4' target=\"_blank\">grid_run_0</a></strong> to <a href='https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid' target=\"_blank\">https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid/runs/76lnj2x4' target=\"_blank\">https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid/runs/76lnj2x4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/20 [Train]: 100%|██████████| 2679/2679 [03:00<00:00, 14.86it/s]\n",
      "Epoch 0/20 [Val]  : 100%|██████████| 574/574 [00:13<00:00, 42.04it/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  → New best validation loss: 2.3203\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>▁</td></tr><tr><td>best_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_val_acc</td><td>0</td></tr><tr><td>best_val_loss</td><td>2.32034</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">grid_run_0</strong> at: <a href='https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid/runs/76lnj2x4' target=\"_blank\">https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid/runs/76lnj2x4</a><br> View project at: <a href='https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid' target=\"_blank\">https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251201_160006-76lnj2x4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/helgamariamagnusdottir/Documents/dtu/deep_learning/DL-group-63-P29/src/models/wandb/run-20251201_160324-ri55atu5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid/runs/ri55atu5' target=\"_blank\">grid_run_1</a></strong> to <a href='https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid' target=\"_blank\">https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid/runs/ri55atu5' target=\"_blank\">https://wandb.ai/ais-maritime-data/dl-maritime-classification-grid/runs/ri55atu5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/20 [Train]:  70%|██████▉   | 1873/2679 [02:13<00:57, 13.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# ---- W&B run (optional but recommended) ----\u001b[39;00m\n\u001b[32m     20\u001b[39m wandb.init(\n\u001b[32m     21\u001b[39m     project=\u001b[33m\"\u001b[39m\u001b[33mdl-maritime-classification-grid\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     22\u001b[39m     config=cfg,\n\u001b[32m     23\u001b[39m     name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mgrid_run_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m     reinit=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     25\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m best_val_loss, best_val_acc = \u001b[43m_train_one_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m wandb.log({\n\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbest_val_loss\u001b[39m\u001b[33m\"\u001b[39m: best_val_loss,\n\u001b[32m     31\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbest_val_acc\u001b[39m\u001b[33m\"\u001b[39m: best_val_acc,\n\u001b[32m     32\u001b[39m })\n\u001b[32m     33\u001b[39m wandb.finish()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36m_train_one_run\u001b[39m\u001b[34m(cfg, train_loader, val_loader)\u001b[39m\n\u001b[32m     34\u001b[39m logits = model(xb)  \u001b[38;5;66;03m# (B, num_classes)\u001b[39;00m\n\u001b[32m     35\u001b[39m loss = crit(logits, yb)\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), cfg[\u001b[33m\"\u001b[39m\u001b[33mgrad_clip\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     38\u001b[39m opt.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._post_run_cell_hook of <wandb.sdk.wandb_init._WandbInit object at 0x168ebdc70>> (for post_run_cell), with arguments args (<ExecutionResult object at 32fb2a7b0, execution_count=10 error_before_exec=None error_in_exec= info=<ExecutionInfo object at 32fb2a960, raw_cell=\"keys = list(param_grid.keys())\n",
      "all_combos = list(p..\" transformed_cell=\"keys = list(param_grid.keys())\n",
      "all_combos = list(p..\" store_history=True silent=False shell_futures=True cell_id=vscode-notebook-cell:/Users/helgamariamagnusdottir/Documents/dtu/deep_learning/DL-group-63-P29/src/models/train_classification_rnn.ipynb#Y104sZmlsZQ%3D%3D> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "ConnectionResetError",
     "evalue": "Connection lost",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/wandb/sdk/wandb_init.py:603\u001b[39m, in \u001b[36m_WandbInit._post_run_cell_hook\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    600\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m._logger.info(\u001b[33m\"\u001b[39m\u001b[33mresuming backend\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/wandb/sdk/interface/interface.py:820\u001b[39m, in \u001b[36mInterfaceBase.publish_resume\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    819\u001b[39m     resume = pb.ResumeRequest()\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish_resume\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresume\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/wandb/sdk/interface/interface_shared.py:334\u001b[39m, in \u001b[36mInterfaceShared._publish_resume\u001b[39m\u001b[34m(self, resume)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_publish_resume\u001b[39m(\u001b[38;5;28mself\u001b[39m, resume: pb.ResumeRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    333\u001b[39m     rec = \u001b[38;5;28mself\u001b[39m._make_request(resume=resume)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_publish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/wandb/sdk/interface/interface_sock.py:46\u001b[39m, in \u001b[36mInterfaceSock._publish\u001b[39m\u001b[34m(self, record, nowait)\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncer.run_soon(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m._client.publish(request))\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_asyncer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpublish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py:136\u001b[39m, in \u001b[36mAsyncioManager.run\u001b[39m\u001b[34m(self, fn)\u001b[39m\n\u001b[32m    133\u001b[39m future = \u001b[38;5;28mself\u001b[39m._schedule(fn, daemon=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m concurrent.futures.CancelledError:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RunCancelledError \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/wandb/sdk/lib/asyncio_manager.py:219\u001b[39m, in \u001b[36mAsyncioManager._wrap\u001b[39m\u001b[34m(self, fn, daemon, name)\u001b[39m\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m (task := asyncio.current_task()):\n\u001b[32m    217\u001b[39m         task.set_name(name)\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m fn()\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    221\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m daemon:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/wandb/sdk/lib/service/service_client.py:38\u001b[39m, in \u001b[36mServiceClient.publish\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpublish\u001b[39m(\u001b[38;5;28mself\u001b[39m, request: spb.ServerRequest) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Send a request without waiting for a response.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_server_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/wandb/sdk/lib/service/service_client.py:64\u001b[39m, in \u001b[36mServiceClient._send_server_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     61\u001b[39m data = request.SerializeToString()\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m._writer.write(data)\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._writer.drain()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/streams.py:392\u001b[39m, in \u001b[36mStreamWriter.drain\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing():\n\u001b[32m    382\u001b[39m     \u001b[38;5;66;03m# Wait for protocol.connection_lost() call\u001b[39;00m\n\u001b[32m    383\u001b[39m     \u001b[38;5;66;03m# Raise connection closing error if any,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    389\u001b[39m     \u001b[38;5;66;03m# in a loop would never call connection_lost(), so it\u001b[39;00m\n\u001b[32m    390\u001b[39m     \u001b[38;5;66;03m# would not see an error when the socket is closed.\u001b[39;00m\n\u001b[32m    391\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m sleep(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol._drain_helper()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.12/3.12.2_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/streams.py:166\u001b[39m, in \u001b[36mFlowControlMixin._drain_helper\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_drain_helper\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection_lost:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionResetError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mConnection lost\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    167\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._paused:\n\u001b[32m    168\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[31mConnectionResetError\u001b[39m: Connection lost"
     ]
    }
   ],
   "source": [
    "# keys = list(param_grid.keys())\n",
    "# all_combos = list(product(*[param_grid[k] for k in keys]))\n",
    "\n",
    "\n",
    "# results = []  # to store (config, best_val_loss, best_val_acc)\n",
    "\n",
    "# print(f\"Total combinations: {len(all_combos)}\")\n",
    "\n",
    "\n",
    "# for i, values in enumerate(all_combos):\n",
    "#     # ---- build config for this combo ----\n",
    "#     cfg = base_config.copy()\n",
    "#     for k, v in zip(keys, values):\n",
    "#         cfg[k] = v\n",
    "\n",
    "#     # make loaders for this batch size\n",
    "#     train_loader, val_loader = make_loaders(cfg[\"batch_size\"])\n",
    "\n",
    "#     # ---- W&B run (optional but recommended) ----\n",
    "#     wandb.init(\n",
    "#         project=\"dl-maritime-classification-grid\",\n",
    "#         config=cfg,\n",
    "#         name=f\"grid_run_{i}\",\n",
    "#         reinit=True,\n",
    "#     )\n",
    "\n",
    "#     best_val_loss, best_val_acc = _train_one_run(wandb.config, train_loader, val_loader)\n",
    "\n",
    "#     wandb.log({\n",
    "#         \"best_val_loss\": best_val_loss,\n",
    "#         \"best_val_acc\": best_val_acc,\n",
    "#     })\n",
    "#     wandb.finish()\n",
    "\n",
    "#     results.append((cfg.copy(), float(best_val_loss), float(best_val_acc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f666fd2",
   "metadata": {},
   "source": [
    "## 3.3 - Save model and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78dfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Save model\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "model_path = os.path.join(MODEL_DIR, \"classification_rnn_model.pt\")\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"\\nModel saved to: {model_path}\")\n",
    "\n",
    "# Optional: save to WandB\n",
    "# if wandb.run is not None:\n",
    "#     wandb.save(model_path)\n",
    "#     wandb.run.summary[\"best_val_loss\"] = best_val_loss\n",
    "\n",
    "# %% Plot training history (optional)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(train_losses, label='Train Loss', marker='o')\n",
    "ax1.plot(val_losses, label='Val Loss', marker='s')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "ax2.plot(train_accs, label='Train Accuracy', marker='o')\n",
    "ax2.plot(val_accs, label='Val Accuracy', marker='s')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training and Validation Accuracy')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# %% Training summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Total epochs: {cfg['epochs']}\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Final train loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"Final train accuracy: {train_accs[-1]:.4f}\")\n",
    "print(f\"Final val loss: {val_losses[-1]:.4f}\")\n",
    "print(f\"Final val accuracy: {val_accs[-1]:.4f}\")\n",
    "print(f\"\\nModel saved to: {model_path}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
