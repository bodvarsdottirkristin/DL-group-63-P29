{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "204ff3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import yaml\n",
    "import itertools\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from trajectory_predictor import TrajectoryPredictor, trajectory_loss, DEVICE\n",
    "from seed import set_seed\n",
    "from config import flatten_config\n",
    "\n",
    "from classification_rnn import ClassificationRNN, DEVICE\n",
    "from seed import set_seed\n",
    "from config import flatten_config\n",
    "\n",
    "BASE = \"../../data/aisdk/processed\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735001dc",
   "metadata": {},
   "source": [
    "# 0 - Fetch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5874cf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_traj = np.load(os.path.join(BASE, \"windows/train_trajectories.npz\"))\n",
    "val_traj = np.load(os.path.join(BASE, \"windows/val_trajectories.npz\"))\n",
    "test_traj = np.load(os.path.join(BASE, \"windows/test_trajectories.npz\"))\n",
    "\n",
    "X_train, X_val = train_traj[\"past\"], val_traj[\"past\"]\n",
    "y_train, y_val = train_traj[\"future\"], val_traj[\"future\"]\n",
    "c_train, c_val = train_traj[\"cluster\"], val_traj[\"cluster\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76f00409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Create data loaders\n",
    "def make_loaders(batch_size, cid):\n",
    "\n",
    "    # Boolean masks for this cluster\n",
    "    train_mask = (c_train == cid)\n",
    "    val_mask   = (c_val == cid)\n",
    "\n",
    "    # Subset numpy arrays\n",
    "    X_tr = X_train[train_mask]\n",
    "    y_tr = y_train[train_mask]\n",
    "    X_v  = X_val[val_mask]\n",
    "    y_v  = y_val[val_mask]\n",
    "    \n",
    "    # %% Convert to PyTorch tensors\n",
    "    X_train_t = torch.tensor(X_tr, dtype=torch.float32)\n",
    "    X_val_t   = torch.tensor(X_v,   dtype=torch.float32)\n",
    "\n",
    "    y_train_t = torch.tensor(y_tr, dtype=torch.float32)\n",
    "    y_val_t   = torch.tensor(y_v,   dtype=torch.float32)\n",
    "\n",
    "    # Create data loaders\n",
    "    print(f\"\\nTensor shapes:\")\n",
    "    print(f\"  X_train_t: {X_train_t.shape}\")\n",
    "    print(f\"  X_val_t:   {X_val_t.shape}\")\n",
    "    print(f\"  y_train_t: {y_train_t.shape}\")\n",
    "    print(f\"  y_val_t:   {y_val_t.shape}\")\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_t, y_train_t), batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_t, y_val_t), batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8ddc34",
   "metadata": {},
   "source": [
    "# 1 - Define training functions \n",
    "## 1.1 - One run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcb6b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_one_run(cfg, train_loader, val_loader, cid):\n",
    "    device = cfg[\"device\"]\n",
    "\n",
    "    model = TrajectoryPredictor(\n",
    "        input_dim=cfg[\"input_dim\"],\n",
    "        hidden_dim=cfg[\"hidden_dim\"],\n",
    "        output_dim=cfg[\"output_dim\"],\n",
    "        num_layers_encoder=cfg[\"num_layers_encoder\"],\n",
    "        num_layers_decoder=cfg[\"num_layers_decoder\"],\n",
    "        attn_dim=cfg[\"attn_dim\"]\n",
    "    ).to(device)\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=cfg[\"lr\"], weight_decay=cfg[\"weight_decay\"])\n",
    "\n",
    "    best_val_mse = float(\"inf\")\n",
    "\n",
    "    # ------ Training loop ------ \n",
    "    train_samples = 0\n",
    "    for epoch in range(1, cfg[\"epochs\"] + 1):\n",
    "\n",
    "        model.train()\n",
    "        total = 0.0\n",
    "\n",
    "        for xb, yb in tqdm(train_loader, desc=f\"Cluster {cid} Epoch {epoch}/{cfg['epochs']}\"):\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "\n",
    "\n",
    "            opt.zero_grad()\n",
    "            yb_pred = model(xb, target_length=yb.size(1), targets=yb, teacher_forcing_ratio=cfg[\"teacher_forcing\"])\n",
    "            \n",
    "            loss = trajectory_loss(yb_pred, yb)\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=cfg[\"max_norm\"])\n",
    "            opt.step()\n",
    "            total += loss.item() * xb.size(0)\n",
    "            train_samples += xb.size(0)\n",
    "\n",
    "\n",
    "        train_mse = total / len(train_loader)\n",
    "\n",
    "\n",
    "        # --- VALIDATION ---\n",
    "        model.eval()\n",
    "        val_total = 0.0\n",
    "        val_samples = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                yb_pred = model(\n",
    "                    xb,\n",
    "                    target_length=yb.size(1),\n",
    "                    targets=None,\n",
    "                    teacher_forcing_ratio=0.0\n",
    "                )\n",
    "\n",
    "                loss = trajectory_loss(yb_pred, yb)\n",
    "                bs = xb.size(0)\n",
    "\n",
    "                val_total += loss * bs\n",
    "                val_samples += bs.size(0)\n",
    "\n",
    "        val_mse = val_total / len(val_loader)\n",
    "        best_val_mse = min(best_val_mse,val_mse)\n",
    "\n",
    "        return best_val_mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abca760",
   "metadata": {},
   "source": [
    "## 1.2 - Function for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9ccee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define key metrics\n",
    "input_dim = X_train.shape[-1]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75d37c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search_trajectory(\n",
    "    device=device,\n",
    "    input_dim=input_dim,\n",
    "    output_dim=input_dim,\n",
    "    cid=1,\n",
    "    search_type='grid',\n",
    "    save_dir='../../checkpoints/hyperparameter_results_trajectory',\n",
    "):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"HYPERPARAMETER TUNING FOR TRAJECTORY PREDICTOR (Cluster {cid})\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # -------- 1) Define search space --------\n",
    "    if search_type == 'grid':\n",
    "        param_grid = {\n",
    "            # main model knobs\n",
    "            \"hidden_dim\": [64, 128],\n",
    "            \"num_layers_encoder\": [1, 2],\n",
    "            \"num_layers_decoder\": [1, 2],\n",
    "            \"attn_dim\": [64, 128],\n",
    "            \"batch_size\": [256],\n",
    "\n",
    "            # training knobs\n",
    "            \"lr\": [1e-4, 3e-4, 1e-3],\n",
    "            \"weight_decay\": [0.0, 1e-4],\n",
    "            \"teacher_forcing\": [0.3, 0.5, 0.7],\n",
    "            \"max_norm\": [1.0],\n",
    "            \"epochs\": [20],\n",
    "        }\n",
    "        # total combos = 2*2*2*2 * 3 * 2 * 3 * 1 * 1 = 288 (you can shrink!)\n",
    "\n",
    "    elif search_type == 'quick':\n",
    "        param_grid = {\n",
    "            \"hidden_dim\": [64, 128],\n",
    "            \"num_layers_encoder\": [1],\n",
    "            \"num_layers_decoder\": [1],\n",
    "            \"attn_dim\": [64],\n",
    "            \"batch_size\": [256],\n",
    "\n",
    "            \"lr\": [3e-4, 1e-3],\n",
    "            \"weight_decay\": [0.0],\n",
    "            \"teacher_forcing\": [0.5],\n",
    "            \"max_norm\": [1.0],\n",
    "\n",
    "            \"epochs\": [10],\n",
    "        }\n",
    "        # much smaller: 2 * 1 * 1 * 1 * 2 * 1 * 1 * 1 * 1 = 4\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown search_type: {search_type}\")\n",
    "\n",
    "    keys = list(param_grid.keys())\n",
    "    values = list(param_grid.values())\n",
    "    combinations = list(itertools.product(*values))\n",
    "    param_combinations = [dict(zip(keys, combo)) for combo in combinations]\n",
    "\n",
    "    print(f\"{search_type.capitalize()} Search: {len(param_combinations)} combinations\")\n",
    "    print(f\"Estimated time: {len(param_combinations) * 5} minutes (VERY rough)\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    results = []\n",
    "    best_score = float(\"inf\")  # here: best_val_mse\n",
    "    best_params = None\n",
    "\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    # -------- 2) Main loop over configs --------\n",
    "    for idx, params in enumerate(param_combinations):\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"Trial {idx + 1}/{len(param_combinations)}  (Cluster {cid})\")\n",
    "        elapsed_min = (datetime.now() - start_time).total_seconds() / 60\n",
    "        print(f\"Time elapsed: {elapsed_min:.1f} min\")\n",
    "        print(\"=\" * 70)\n",
    "\n",
    "        print(\"Testing configuration:\")\n",
    "        print(f\"  hidden_dim        = {params['hidden_dim']}\")\n",
    "        print(f\"  num_layers_enc    = {params['num_layers_encoder']}\")\n",
    "        print(f\"  num_layers_dec    = {params['num_layers_decoder']}\")\n",
    "        print(f\"  attn_dim          = {params['attn_dim']}\")\n",
    "        print(f\"  batch_size        = {params['batch_size']}\")\n",
    "        print(f\"  lr                = {params['lr']}\")\n",
    "        print(f\"  weight_decay      = {params['weight_decay']}\")\n",
    "        print(f\"  teacher_forcing   = {params['teacher_forcing']}\")\n",
    "        print(f\"  max_norm          = {params['max_norm']}\")\n",
    "        print(f\"  epochs            = {params['epochs']}\")\n",
    "        print()\n",
    "\n",
    "        try:\n",
    "            # ---- Build cfg for this run ----\n",
    "            cfg = {\n",
    "                \"device\": device,\n",
    "                \"input_dim\": input_dim,\n",
    "                \"output_dim\": output_dim,\n",
    "                \"hidden_dim\": params[\"hidden_dim\"],\n",
    "                \"num_layers_encoder\": params[\"num_layers_encoder\"],\n",
    "                \"num_layers_decoder\": params[\"num_layers_decoder\"],\n",
    "                \"attn_dim\": params[\"attn_dim\"],\n",
    "                \"batch_size\": params[\"batch_size\"],\n",
    "                \"lr\": params[\"lr\"],\n",
    "                \"weight_decay\": params[\"weight_decay\"],\n",
    "                \"teacher_forcing\": params[\"teacher_forcing\"],\n",
    "                \"max_norm\": params[\"max_norm\"],\n",
    "                \"epochs\": params[\"epochs\"],\n",
    "            }\n",
    "\n",
    "            # ---- Train once with this config ----\n",
    "            train_loader, val_loader = make_loaders(cfg[\"batch_size\"], cid)\n",
    "            best_val_mse = _train_one_run(cfg, train_loader, val_loader, cid)\n",
    "\n",
    "            score = float(best_val_mse)\n",
    "\n",
    "            result = {\n",
    "                **params,\n",
    "                \"best_val_mse\": score,\n",
    "                \"score\": score,  # for consistency with other scripts\n",
    "                \"trial\": idx + 1,\n",
    "            }\n",
    "            results.append(result)\n",
    "\n",
    "            print(f\"Results:\")\n",
    "            print(f\"  Best Val MSE: {best_val_mse:.6f}\")\n",
    "            print(f\"  Score (Val MSE): {score:.6f}\")\n",
    "\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_params = params.copy()\n",
    "                print(\"  ⭐ NEW BEST CONFIG FOUND!\")\n",
    "                # If you later change _train_one_run to also return a model,\n",
    "                # you can save the model here.\n",
    "\n",
    "            # Optionally clear cache if GPU:\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ FAILED: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "            result = {**params, \"error\": str(e), \"trial\": idx + 1}\n",
    "            results.append(result)\n",
    "\n",
    "    # -------- 3) Save & summarize --------\n",
    "    if len(results) == 0:\n",
    "        print(\"\\n❌ No trials completed!\")\n",
    "        return None, None\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "\n",
    "    if \"score\" in df.columns:\n",
    "        df = df.sort_values(\"score\", na_position=\"last\")\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    csv_path = os.path.join(save_dir, f\"results_cluster{cid}_{timestamp}.csv\")\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    if best_params is None:\n",
    "        print(\"\\n❌ All trials failed! Check error messages above.\")\n",
    "        return df, None\n",
    "\n",
    "    json_path = os.path.join(save_dir, f\"best_params_cluster{cid}_{timestamp}.json\")\n",
    "    with open(json_path, \"w\") as f:\n",
    "        json.dump(best_params, f, indent=2)\n",
    "\n",
    "    total_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"HYPERPARAMETER SEARCH COMPLETE (Cluster {cid})\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total time: {total_time:.1f} minutes\")\n",
    "    successful = [r for r in results if \"error\" not in r]\n",
    "    print(f\"Successful trials: {len(successful)}/{len(results)}\")\n",
    "\n",
    "    print(f\"\\nBest parameters (score={best_score:.6f}):\")\n",
    "    for k, v in best_params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    print(f\"\\nResults saved to: {csv_path}\")\n",
    "    print(f\"Best params saved to: {json_path}\")\n",
    "\n",
    "    # Top 5 successful configs\n",
    "    successful_df = df[df[\"score\"].notna()]\n",
    "    top_cols = [\n",
    "        \"hidden_dim\",\n",
    "        \"num_layers_encoder\",\n",
    "        \"num_layers_decoder\",\n",
    "        \"attn_dim\",\n",
    "        \"lr\",\n",
    "        \"teacher_forcing\",\n",
    "        \"weight_decay\",\n",
    "        \"best_val_mse\",\n",
    "        \"score\",\n",
    "    ]\n",
    "    print(\"\\nTop 5 configurations:\")\n",
    "    print(successful_df[top_cols].head())\n",
    "\n",
    "    return df, best_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef35e5",
   "metadata": {},
   "source": [
    "# 2 - Fetch the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2df93b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HYPERPARAMETER TUNING FOR TRAJECTORY PREDICTOR (Cluster 1)\n",
      "======================================================================\n",
      "Quick Search: 4 combinations\n",
      "Estimated time: 20 minutes (VERY rough)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Trial 1/4  (Cluster 1)\n",
      "Time elapsed: 0.0 min\n",
      "======================================================================\n",
      "Testing configuration:\n",
      "  hidden_dim        = 64\n",
      "  num_layers_enc    = 1\n",
      "  num_layers_dec    = 1\n",
      "  attn_dim          = 64\n",
      "  batch_size        = 256\n",
      "  lr                = 0.0003\n",
      "  weight_decay      = 0.0\n",
      "  teacher_forcing   = 0.5\n",
      "  max_norm          = 1.0\n",
      "  epochs            = 10\n",
      "\n",
      "\n",
      "Tensor shapes:\n",
      "  X_train_t: torch.Size([16725, 30, 5])\n",
      "  X_val_t:   torch.Size([5235, 30, 5])\n",
      "  y_train_t: torch.Size([16725, 30, 5])\n",
      "  y_val_t:   torch.Size([5235, 30, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cluster 1 Epoch 1/10:  24%|██▍       | 16/66 [00:05<00:16,  3.12it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m output_dim = \u001b[32m5\u001b[39m         \u001b[38;5;66;03m# same features for future sequence\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Train cluster 1 for example\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df_traj, best_params_traj = \u001b[43mhyperparameter_search_trajectory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcid\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43msearch_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquick\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# or \"grid\"\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 110\u001b[39m, in \u001b[36mhyperparameter_search_trajectory\u001b[39m\u001b[34m(device, input_dim, output_dim, cid, search_type, save_dir)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;66;03m# ---- Train once with this config ----\u001b[39;00m\n\u001b[32m    109\u001b[39m train_loader, val_loader = make_loaders(cfg[\u001b[33m\"\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m\"\u001b[39m], cid)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m best_val_mse = \u001b[43m_train_one_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m score = \u001b[38;5;28mfloat\u001b[39m(best_val_mse)\n\u001b[32m    114\u001b[39m result = {\n\u001b[32m    115\u001b[39m     **params,\n\u001b[32m    116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbest_val_mse\u001b[39m\u001b[33m\"\u001b[39m: score,\n\u001b[32m    117\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m: score,  \u001b[38;5;66;03m# for consistency with other scripts\u001b[39;00m\n\u001b[32m    118\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtrial\u001b[39m\u001b[33m\"\u001b[39m: idx + \u001b[32m1\u001b[39m,\n\u001b[32m    119\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36m_train_one_run\u001b[39m\u001b[34m(cfg, train_loader, val_loader, cid)\u001b[39m\n\u001b[32m     30\u001b[39m yb_pred = model(xb, target_length=yb.size(\u001b[32m1\u001b[39m), targets=yb, teacher_forcing_ratio=cfg[\u001b[33m\"\u001b[39m\u001b[33mteacher_forcing\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     32\u001b[39m loss = trajectory_loss(yb_pred, yb)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m nn.utils.clip_grad_norm_(model.parameters(), max_norm=cfg[\u001b[33m\"\u001b[39m\u001b[33mmax_norm\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     36\u001b[39m opt.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/dtu/deep_learning/DL-group-63-P29/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "input_dim = 5          # e.g. [UTM_x, UTM_y, SOG, v_east, v_north]\n",
    "output_dim = 5         # same features for future sequence\n",
    "\n",
    "# Train cluster 1 for example\n",
    "df_traj, best_params_traj = hyperparameter_search_trajectory(\n",
    "    device=device,\n",
    "    input_dim=input_dim,\n",
    "    output_dim=output_dim,\n",
    "    cid=1,\n",
    "    search_type=\"quick\",   # or \"grid\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
