{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4fc2d60",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a16066",
   "metadata": {},
   "source": [
    "## 1 - Load Data and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f6b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "BASE = \"../../data/aisdk/processed\"\n",
    "TEST_DATA_PATH = os.path.join(BASE, \"windows/test_trajectories.npz\")\n",
    "MODELS_DIR = \"../../checkpoints/trained_models\"\n",
    "\n",
    "# Load test data\n",
    "print(\"Loading test data...\")\n",
    "test_data = np.load(TEST_DATA_PATH)\n",
    "X_test = test_data[\"past\"]           # (N, 30, 5) - past trajectories\n",
    "y_test = test_data[\"future\"]         # (N, 30, 5) - future trajectories\n",
    "c_test = test_data[\"cluster\"]        # (N,) - cluster labels\n",
    "\n",
    "print(f\"Test set shapes:\")\n",
    "print(f\"  X_test (past):   {X_test.shape}\")\n",
    "print(f\"  y_test (future): {y_test.shape}\")\n",
    "print(f\"  c_test (labels): {c_test.shape}\")\n",
    "print(f\"\\nUnique clusters: {np.unique(c_test)}\")\n",
    "print(f\"Cluster distribution:\\n{pd.Series(c_test).value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f1d408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model classes\n",
    "import sys\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from classification_rnn import ClassificationRNN\n",
    "from trajectory_predictor import TrajectoryPredictor, trajectory_loss\n",
    "\n",
    "# Model paths (update these with your actual model paths)\n",
    "classification_model_path = os.path.join(MODELS_DIR, \"classification_rnn_best.pt\")\n",
    "trajectory_model_dir = os.path.join(MODELS_DIR, \"trajectory_predictors\")  # per-cluster models\n",
    "\n",
    "print(f\"Classification model path: {classification_model_path}\")\n",
    "print(f\"Trajectory models dir: {trajectory_model_dir}\")\n",
    "print(f\"\\nModel paths exist:\")\n",
    "print(f\"  Classification: {os.path.exists(classification_model_path)}\")\n",
    "print(f\"  Trajectory dir: {os.path.isdir(trajectory_model_dir)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe79cf5a",
   "metadata": {},
   "source": [
    "## 2 - Classification RNN Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ac041",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CLASSIFICATION RNN EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load classification model\n",
    "num_clusters = len(np.unique(c_test))\n",
    "input_dim = X_test.shape[-1]\n",
    "\n",
    "classifier = ClassificationRNN(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=64,\n",
    "    num_layers=1,\n",
    "    output_dim=num_clusters\n",
    ").to(device)\n",
    "\n",
    "# Load weights\n",
    "classifier.load_state_dict(torch.load(classification_model_path, map_location=device))\n",
    "classifier.eval()\n",
    "print(f\"-> Loaded classification model\")\n",
    "\n",
    "# Run inference\n",
    "print(\"\\nRunning inference on test set...\")\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "c_test_t = torch.tensor(c_test, dtype=torch.long).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = classifier(X_test_t)  # (N, num_clusters)\n",
    "    c_pred = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "    probs = torch.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "print(f\"Predictions shape: {c_pred.shape}\")\n",
    "print(f\"Probabilities shape: {probs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28490c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "accuracy = accuracy_score(c_test, c_pred)\n",
    "precision = precision_score(c_test, c_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(c_test, c_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(c_test, c_pred, average='weighted', zero_division=0)\n",
    "\n",
    "print(f\"\\n Classification Metrics:\")\n",
    "print(f\"  Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"  Precision: {precision:.4f}\")\n",
    "print(f\"  Recall:    {recall:.4f}\")\n",
    "print(f\"  F1-Score:  {f1:.4f}\")\n",
    "\n",
    "# Per-cluster metrics\n",
    "print(f\"\\n Per-Cluster Performance:\")\n",
    "for cid in np.unique(c_test):\n",
    "    mask = c_test == cid\n",
    "    acc = accuracy_score(c_test[mask], c_pred[mask])\n",
    "    prec = precision_score(c_test[mask], c_pred[mask], average='weighted', zero_division=0)\n",
    "    rec = recall_score(c_test[mask], c_pred[mask], average='weighted', zero_division=0)\n",
    "    f1_c = f1_score(c_test[mask], c_pred[mask], average='weighted', zero_division=0)\n",
    "    count = mask.sum()\n",
    "    print(f\"  Cluster {cid}: Acc={acc:.3f}, Prec={prec:.3f}, Rec={rec:.3f}, F1={f1_c:.3f} (n={count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e29065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(c_test, c_pred)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar_kws={'label': 'Count'})\n",
    "ax.set_xlabel('Predicted Cluster')\n",
    "ax.set_ylabel('True Cluster')\n",
    "ax.set_title('Classification RNN: Confusion Matrix on Test Set')\n",
    "plt.tight_layout()\n",
    "plt.savefig('classification_confusion_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"-> Confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2eb66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction confidence analysis\n",
    "max_probs = probs.max(axis=1)\n",
    "correct = (c_pred == c_test)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confidence distribution\n",
    "axes[0].hist(max_probs[correct], bins=30, alpha=0.7, label='Correct', color='green')\n",
    "axes[0].hist(max_probs[~correct], bins=30, alpha=0.7, label='Incorrect', color='red')\n",
    "axes[0].set_xlabel('Max Probability')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_title('Prediction Confidence Distribution')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Accuracy vs confidence\n",
    "confidence_bins = np.linspace(0, 1, 11)\n",
    "bin_accs = []\n",
    "bin_counts = []\n",
    "for i in range(len(confidence_bins)-1):\n",
    "    mask = (max_probs >= confidence_bins[i]) & (max_probs < confidence_bins[i+1])\n",
    "    if mask.sum() > 0:\n",
    "        bin_acc = (c_pred[mask] == c_test[mask]).mean()\n",
    "        bin_accs.append(bin_acc)\n",
    "        bin_counts.append(mask.sum())\n",
    "    else:\n",
    "        bin_accs.append(0)\n",
    "        bin_counts.append(0)\n",
    "\n",
    "bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2\n",
    "axes[1].bar(bin_centers, bin_accs, width=0.08, color='steelblue', alpha=0.7)\n",
    "axes[1].axhline(y=accuracy, color='r', linestyle='--', label=f'Overall Accuracy: {accuracy:.3f}')\n",
    "axes[1].set_xlabel('Confidence Bin')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Accuracy vs Prediction Confidence')\n",
    "axes[1].set_ylim([0, 1.05])\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('classification_confidence_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"->Confidence analysis saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a3607",
   "metadata": {},
   "source": [
    "## 3 - Trajectory Predictor Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d068d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAJECTORY PREDICTOR EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load per-cluster trajectory models\n",
    "trajectory_models = {}\n",
    "print(\"\\nLoading per-cluster trajectory models...\")\n",
    "\n",
    "for cid in np.unique(c_test):\n",
    "    model_path = os.path.join(trajectory_model_dir, f\"trajectory_cluster_{cid}.pt\")\n",
    "    \n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Cluster {cid}: Model not found at {model_path}\")\n",
    "        continue\n",
    "    \n",
    "    # Create model\n",
    "    traj_model = TrajectoryPredictor(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=64,\n",
    "        output_dim=input_dim,\n",
    "        num_layers_encoder=1,\n",
    "        num_layers_decoder=1,\n",
    "        attn_dim=64\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load weights\n",
    "    traj_model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    traj_model.eval()\n",
    "    trajectory_models[cid] = traj_model\n",
    "    print(f\"-> Cluster {cid} model loaded\")\n",
    "\n",
    "print(f\"\\nLoaded {len(trajectory_models)} trajectory models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03493a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run trajectory predictions\n",
    "print(\"\\nRunning trajectory predictions on test set...\")\n",
    "\n",
    "y_pred_list = []\n",
    "valid_indices = []\n",
    "\n",
    "for i, (X_i, y_i, c_i) in enumerate(zip(X_test, y_test, c_test)):\n",
    "    if c_i not in trajectory_models:\n",
    "        continue  # Skip if model is not available\n",
    "    \n",
    "    model = trajectory_models[c_i]\n",
    "    X_i_t = torch.tensor(X_i, dtype=torch.float32).unsqueeze(0).to(device)  # (1, 30, 5)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        y_pred_i = model(\n",
    "            X_i_t,\n",
    "            target_length=30,\n",
    "            targets=None,\n",
    "            teacher_forcing_ratio=0.0  # No teacher forcing for inference\n",
    "        )  # (1, 30, 5)\n",
    "    \n",
    "    y_pred_list.append(y_pred_i.cpu().numpy().squeeze())\n",
    "    valid_indices.append(i)\n",
    "\n",
    "y_pred = np.array(y_pred_list)  # (M, 30, 5) where M <= N\n",
    "y_test_valid = y_test[valid_indices]\n",
    "c_test_valid = c_test[valid_indices]\n",
    "\n",
    "print(f\"\\nPredictions computed for {len(y_pred)}/{len(y_test)} samples\")\n",
    "print(f\"  y_pred shape: {y_pred.shape}\")\n",
    "print(f\"  y_test_valid shape: {y_test_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e204fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute trajectory metrics\n",
    "def compute_trajectory_metrics(y_true, y_pred):\n",
    "    \"\"\"Compute trajectory prediction metrics.\"\"\"\n",
    "    mse = np.mean((y_true - y_pred) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    \n",
    "    # Per-timestep error\n",
    "    timestep_mse = np.mean((y_true - y_pred) ** 2, axis=(0, 2))  # (30,)\n",
    "    \n",
    "    return mse, rmse, mae, timestep_mse\n",
    "\n",
    "# Overall metrics\n",
    "mse, rmse, mae, ts_mse = compute_trajectory_metrics(y_test_valid, y_pred)\n",
    "\n",
    "print(f\"\\n Trajectory Prediction Metrics (Test Set):\")\n",
    "print(f\"MSE:  {mse:.6f}\")\n",
    "print(f\"RMSE: {rmse:.6f}\")\n",
    "print(f\"MAE:  {mae:.6f}\")\n",
    "\n",
    "# Per-cluster metrics\n",
    "print(f\"\\n Per-Cluster Trajectory Performance:\")\n",
    "for cid in np.unique(c_test_valid):\n",
    "    mask = c_test_valid == cid\n",
    "    c_mse, c_rmse, c_mae, _ = compute_trajectory_metrics(y_test_valid[mask], y_pred[mask])\n",
    "    count = mask.sum()\n",
    "    print(f\"  Cluster {cid}: MSE={c_mse:.6f}, RMSE={c_rmse:.6f}, MAE={c_mae:.6f} (n={count})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c318dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-timestep error analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Timestep MSE\n",
    "axes[0, 0].plot(ts_mse, marker='o', linewidth=2, markersize=4)\n",
    "axes[0, 0].set_xlabel('Timestep (minutes)')\n",
    "axes[0, 0].set_ylabel('MSE')\n",
    "axes[0, 0].set_title('Prediction Error by Timestep')\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Per-feature error\n",
    "feature_names = ['utm_x', 'utm_y', 'SOG', 'v_east', 'v_north']\n",
    "feature_mse = np.mean((y_test_valid - y_pred) ** 2, axis=(0, 1))  # (5,)\n",
    "axes[0, 1].bar(range(len(feature_names)), feature_mse, color='steelblue', alpha=0.7)\n",
    "axes[0, 1].set_xticks(range(len(feature_names)))\n",
    "axes[0, 1].set_xticklabels(feature_names, rotation=45)\n",
    "axes[0, 1].set_ylabel('MSE')\n",
    "axes[0, 1].set_title('Error by Feature')\n",
    "axes[0, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Error distribution\n",
    "errors = np.sqrt(np.mean((y_test_valid - y_pred) ** 2, axis=(1, 2)))  # (M,) - RMSE per sample\n",
    "axes[1, 0].hist(errors, bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].axvline(rmse, color='r', linestyle='--', linewidth=2, label=f'Mean RMSE: {rmse:.4f}')\n",
    "axes[1, 0].set_xlabel('RMSE per Sample')\n",
    "axes[1, 0].set_ylabel('Count')\n",
    "axes[1, 0].set_title('Distribution of Prediction Errors')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "# Per-cluster comparison\n",
    "cluster_mses = []\n",
    "cluster_labels = []\n",
    "for cid in np.unique(c_test_valid):\n",
    "    mask = c_test_valid == cid\n",
    "    c_mse, _, _, _ = compute_trajectory_metrics(y_test_valid[mask], y_pred[mask])\n",
    "    cluster_mses.append(c_mse)\n",
    "    cluster_labels.append(f'C{cid}')\n",
    "\n",
    "axes[1, 1].bar(range(len(cluster_labels)), cluster_mses, color='steelblue', alpha=0.7)\n",
    "axes[1, 1].set_xticks(range(len(cluster_labels)))\n",
    "axes[1, 1].set_xticklabels(cluster_labels)\n",
    "axes[1, 1].set_ylabel('MSE')\n",
    "axes[1, 1].set_title('Prediction Error by Cluster')\n",
    "axes[1, 1].grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('trajectory_error_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"-> Error analysis saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a4e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions\n",
    "num_samples = 6\n",
    "fig, axes = plt.subplots(num_samples, 5, figsize=(16, 12))\n",
    "\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(y_pred), num_samples, replace=False)\n",
    "\n",
    "for row, idx in enumerate(sample_indices):\n",
    "    y_true_sample = y_test_valid[idx]  # (30, 5)\n",
    "    y_pred_sample = y_pred[idx]        # (30, 5)\n",
    "    \n",
    "    for col in range(5):\n",
    "        ax = axes[row, col]\n",
    "        timesteps = np.arange(30)\n",
    "        \n",
    "        ax.plot(timesteps, y_true_sample[:, col], 'o-', label='True', linewidth=2, markersize=4)\n",
    "        ax.plot(timesteps, y_pred_sample[:, col], 's--', label='Pred', linewidth=2, markersize=4)\n",
    "        \n",
    "        if row == 0:\n",
    "            ax.set_title(feature_names[col], fontsize=12, fontweight='bold')\n",
    "        if col == 0:\n",
    "            ax.set_ylabel(f'Sample {idx}', fontsize=11)\n",
    "        if row == num_samples - 1:\n",
    "            ax.set_xlabel('Timestep', fontsize=10)\n",
    "        \n",
    "        ax.grid(alpha=0.3)\n",
    "        if row == 0 and col == 4:\n",
    "            ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('trajectory_sample_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"-> Sample predictions visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3841e79b",
   "metadata": {},
   "source": [
    "## 4 - Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bce249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive report\n",
    "report = f\"\"\"\n",
    "{'='*70}\n",
    "FINAL MODEL EVALUATION REPORT\n",
    "{'='*70}\n",
    "\n",
    "TEST SET OVERVIEW\n",
    "{'-'*70}\n",
    "Total samples:        {len(X_test)}\n",
    "Test samples used:    {len(y_pred)}\n",
    "Features:             {input_dim}\n",
    "Sequence length:      30 timesteps\n",
    "Unique clusters:      {len(np.unique(c_test))}\n",
    "\n",
    "CLASSIFICATION RNN RESULTS\n",
    "{'-'*70}\n",
    "Accuracy:             {accuracy:.4f} ({accuracy*100:.2f}%)\n",
    "Precision (weighted): {precision:.4f}\n",
    "Recall (weighted):    {recall:.4f}\n",
    "F1-Score (weighted):  {f1:.4f}\n",
    "\n",
    "TRAJECTORY PREDICTOR RESULTS\n",
    "{'-'*70}\n",
    "Mean Squared Error:   {mse:.6f}\n",
    "Root Mean Sq. Error:  {rmse:.6f}\n",
    "Mean Absolute Error:  {mae:.6f}\n",
    "Best timestep:        T={np.argmin(ts_mse)} (MSE={np.min(ts_mse):.6f})\n",
    "Worst timestep:       T={np.argmax(ts_mse)} (MSE={np.max(ts_mse):.6f})\n",
    "\n",
    "FEATURE-WISE PREDICTION ERROR\n",
    "{'-'*70}\n",
    "\"\"\"\n",
    "\n",
    "for i, fname in enumerate(feature_names):\n",
    "    report += f\"{fname:12s}: MSE={feature_mse[i]:.6f}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "VISUALIZATIONS GENERATED\n",
    "{'-'*70}\n",
    "-> classification_confusion_matrix.png\n",
    "-> classification_confidence_analysis.png\n",
    "-> trajectory_error_analysis.png\n",
    "-> trajectory_sample_predictions.png\n",
    "\n",
    "INTERPRETATION NOTES\n",
    "{'-'*70}\n",
    "• Classification accuracy {('GOOD' if accuracy > 0.7 else 'MODERATE' if accuracy > 0.5 else 'POOR')} ({accuracy:.1%})\n",
    "• Trajectory RMSE: {rmse:.6f} (units: meters for spatial, m/s for velocity)\n",
    "• Prediction quality {'improves' if ts_mse[-1] < ts_mse[0] else 'degrades'} over prediction horizon\n",
    "• Error is relatively {'low' if rmse < 50 else 'moderate' if rmse < 100 else 'high'} for maritime data\n",
    "\n",
    "{'='*70}\n",
    "\"\"\"\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Save report\n",
    "with open('test_report.txt', 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(\"\\n-> Report saved to test_report.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51039919",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save detailed metrics to CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'MSE', 'RMSE', 'MAE'],\n",
    "    'Value': [accuracy, precision, recall, f1, mse, rmse, mae]\n",
    "})\n",
    "\n",
    "results_df.to_csv('test_metrics.csv', index=False)\n",
    "print(\"-> Metrics saved to test_metrics.csv\")\n",
    "print(\"\\nFinal Metrics Summary:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
