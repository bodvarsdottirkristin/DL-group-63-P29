{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0204f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import RAE\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import plotly.express as px\n",
    "from pyproj import Transformer\n",
    "\n",
    "import pyarrow\n",
    "import pyarrow.parquet\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "54d7d3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_trajectories(df, color_by='MMSI', title=None, zoom=5, height=800):\n",
    "    \n",
    "    transformer = Transformer.from_crs(\"EPSG:25832\", \"EPSG:4326\", always_xy=True)\n",
    "    lon, lat = transformer.transform(df['UTM_x'].values, df['UTM_y'].values)\n",
    "    \n",
    "    if 'Trajectory' in df.columns:\n",
    "        vis_cols = ['Trajectory', 'MMSI', 'Timestamp', 'UTM_x', 'UTM_y']\n",
    "    else:\n",
    "        vis_cols = ['MMSI', 'Timestamp', 'UTM_x', 'UTM_y']\n",
    "\n",
    "    if color_by not in vis_cols:\n",
    "        vis_cols.append(color_by)\n",
    "\n",
    "    if 'SOG' in df.columns and 'SOG' not in vis_cols:\n",
    "        vis_cols.append('SOG')\n",
    "    \n",
    "    vis_df = df[vis_cols].copy()\n",
    "    vis_df['Longitude'] = lon\n",
    "    vis_df['Latitude'] = lat\n",
    "    \n",
    "    # Generate title if not provided\n",
    "    if title is None:\n",
    "        date_min = vis_df['Timestamp'].min().date()\n",
    "        date_max = vis_df['Timestamp'].max().date()\n",
    "        title = f\"Ship Trajectories - {date_min} to {date_max}\"\n",
    "    \n",
    "    # Create visualization with trajectories using lat/lon on a map\n",
    "    fig = px.line_map(\n",
    "        vis_df.sort_values('Timestamp'),\n",
    "        lat=\"Latitude\",\n",
    "        lon=\"Longitude\",\n",
    "        color=color_by,\n",
    "        line_group=\"Trajectory\",\n",
    "        hover_data=[\"MMSI\", \"Timestamp\", \"SOG\"] if \"SOG\" in vis_df.columns else [\"MMSI\", \"Timestamp\"],\n",
    "        zoom=zoom,\n",
    "        title=title\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        mapbox_style=\"open-street-map\",\n",
    "        showlegend=False,  # Hide legend since there can be many trajectories\n",
    "        height=height\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ“ Visualization complete - colored by '{color_by}'\")\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f7f9163a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "feature_size = 5\n",
    "hidden_size = 64\n",
    "latent_dim = 12\n",
    "encoder_layers = 3\n",
    "decoder_layers = 2\n",
    "dropout = 0.2\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ee00c7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded StandardScaler object from joblib.\n",
      "Scaler ready. n_features: 5 | feature names: None\n"
     ]
    }
   ],
   "source": [
    "obj = joblib.load(os.path.join(\"..\", \"..\", \"data\", \"aisdk\", \"scaler\", \"scaler_aisdk_2025.pkl\"))\n",
    "\n",
    "saved_feature_names = None\n",
    "\n",
    "if isinstance(obj, StandardScaler):\n",
    "    scaler = obj\n",
    "    # If you stored feature names separately, keep as None here\n",
    "    print(\"Loaded StandardScaler object from joblib.\")\n",
    "elif isinstance(obj, dict):\n",
    "    scaler = StandardScaler()\n",
    "    scaler.mean_ = np.asarray(obj['mean'])\n",
    "    scaler.scale_ = np.asarray(obj['scale'])\n",
    "    saved_feature_names = obj.get('feature_names', None)\n",
    "    print(\"Reconstructed StandardScaler from dict params.\")\n",
    "else:\n",
    "    raise TypeError(f\"Unexpected scaler payload type: {type(obj)}\")\n",
    "\n",
    "print(\"Scaler ready. n_features:\", scaler.mean_.shape[0],\n",
    "      \"| feature names:\", (list(saved_feature_names) if saved_feature_names is not None else \"None\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e7039fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un-scale train and val DataFrames for visualization\n",
    "def inverse_scale_df(df, scaler, cols, saved_feature_names=None):\n",
    "    cols = [c for c in cols if c in df.columns]\n",
    "    if not cols:\n",
    "        return df.copy(), []\n",
    "    Xs = df[cols].to_numpy()\n",
    "    mean = np.asarray(scaler.mean_)\n",
    "    scale = np.asarray(scaler.scale_)\n",
    "    # Align mean/scale to column order if saved feature names exist\n",
    "    if saved_feature_names is not None:\n",
    "        name_to_idx = {name: i for i, name in enumerate(saved_feature_names)}\n",
    "        idx = []\n",
    "        for c in cols:\n",
    "            if c in name_to_idx:\n",
    "                idx.append(name_to_idx[c])\n",
    "            else:\n",
    "                # Fallback: assume same order for missing names\n",
    "                idx.append(cols.index(c))\n",
    "        mean_aligned = mean[idx]\n",
    "        scale_aligned = scale[idx]\n",
    "    else:\n",
    "        # Assume the first N features correspond to the selected columns\n",
    "        mean_aligned = mean[:len(cols)]\n",
    "        scale_aligned = scale[:len(cols)]\n",
    "    X = Xs * scale_aligned + mean_aligned\n",
    "    out = df.copy()\n",
    "    out.loc[:, cols] = X\n",
    "    return out, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccdc7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded! Best val_loss: 1.5638\n",
      "Trained for 30 epochs\n"
     ]
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "checkpoint = torch.load('../../checkpoints/rae/best_rae_model.pth')\n",
    "\n",
    "# Recreate the model architecture\n",
    "model = RAE.RecurrentAutoencoder(\n",
    "    input_dim=feature_size,\n",
    "    hidden_dim=hidden_size,\n",
    "    latent_dim=latent_dim,\n",
    "    num_layers_encoder=encoder_layers,\n",
    "    num_layers_decoder=decoder_layers,\n",
    "    dropout=dropout\n",
    "    ).to(device)\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(f\"Model loaded! Best val_loss: {checkpoint['val_loss']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9b404126",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_parquet('../../data/aisdk/train/aisdk_2025')\n",
    "val = pd.read_parquet('../../data/aisdk/val/aisdk_2025')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "65c20c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training trajectories: 2160\n",
      "Total validationtrajectories: 463\n"
     ]
    }
   ],
   "source": [
    "train_trajectories = []\n",
    "    \n",
    "for traj_id in train['Trajectory'].unique():\n",
    "    traj_data = train[train['Trajectory'] == traj_id].sort_values('Timestamp')\n",
    "    features = traj_data[['UTM_x', 'UTM_y', 'SOG', 'v_east', 'v_north']].values\n",
    "    train_trajectories.append(features)\n",
    "\n",
    "print(f\"Total training trajectories: {len(train_trajectories)}\")\n",
    "\n",
    "val_trajectories = []\n",
    "    \n",
    "for traj_id in val['Trajectory'].unique():\n",
    "    traj_data = val[val['Trajectory'] == traj_id].sort_values('Timestamp')\n",
    "    features = traj_data[['UTM_x', 'UTM_y', 'SOG', 'v_east', 'v_north']].values\n",
    "    val_trajectories.append(features)\n",
    "\n",
    "print(f\"Total validationtrajectories: {len(val_trajectories)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8d1b1112",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, trajectories):\n",
    "        self.trajectories = trajectories\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.trajectories)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        traj = torch.FloatTensor(self.trajectories[idx])\n",
    "        return traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "acff5d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TrajectoryDataset(train_trajectories)\n",
    "val_dataset = TrajectoryDataset(val_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8d9e7e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trajectories(batch):\n",
    "    lengths = torch.tensor([len(traj) for traj in batch])\n",
    "    padded = pad_sequence(batch, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    lengths, perm_idx = lengths.sort(descending=True)\n",
    "    padded = padded[perm_idx]\n",
    "\n",
    "    return padded, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e2d6f84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded train latents: (2160, 12)\n",
      "Encoded val latents: (463, 12)\n",
      "Encoded val latents: (463, 12)\n"
     ]
    }
   ],
   "source": [
    "encode_loader_t = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_trajectories\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "all_train_latents = []\n",
    "with torch.no_grad():\n",
    "    for batch, lengths in encode_loader_t:\n",
    "        batch = batch.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        z = model.encode(batch, lengths)  # [batch_size, latent_dim]\n",
    "        all_train_latents.append(z.cpu())\n",
    "\n",
    "train_latents = torch.cat(all_train_latents, dim=0).numpy()  # shape: [N_trajectories, latent_dim]\n",
    "print(\"Encoded train latents:\", train_latents.shape)\n",
    "\n",
    "encode_loader_v = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_trajectories\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "all_val_latents = []\n",
    "with torch.no_grad():\n",
    "    for batch, lengths in encode_loader_v:\n",
    "        batch = batch.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        z = model.encode(batch, lengths)  # [batch_size, latent_dim]\n",
    "        all_val_latents.append(z.cpu())\n",
    "        \n",
    "val_latents = torch.cat(all_val_latents, dim=0).numpy()  # shape: [N_trajectories, latent_dim]\n",
    "print(\"Encoded val latents:\", val_latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f23b89f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_clusters = 50\n",
    "min_samples = 10\n",
    "metric = 'euclidean'\n",
    "csm = 'eom'\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_clusters,   \n",
    "        min_samples=min_samples,   \n",
    "        metric=metric,\n",
    "        cluster_selection_method=csm,\n",
    "        prediction_data=True,\n",
    "    )\n",
    "\n",
    "t_labels = clusterer.fit_predict(train_latents)\n",
    "\n",
    "# Create df with trajectory and cluster labels\n",
    "traj_ids = list(train['Trajectory'].unique())\n",
    "train_labels_df = pd.DataFrame({\n",
    "    'Trajectory': traj_ids,\n",
    "    'ClusterLabel': t_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "791d5150",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labelled = train.merge(train_labels_df, on='Trajectory', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7add7484",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_labels, v_probs = hdbscan.approximate_predict(clusterer, val_latents)\n",
    "\n",
    "traj_ids = list(val['Trajectory'].unique())\n",
    "\n",
    "# Sanity check: lengths should match\n",
    "assert len(traj_ids) == len(v_labels), f\"Mismatch: {len(traj_ids)} traj_ids vs {len(v_labels)} labels\"\n",
    "\n",
    "val_labels_df = pd.DataFrame({\n",
    "    'Trajectory': traj_ids,\n",
    "    'ClusterLabel': v_labels,\n",
    "    'ClusterProb': v_probs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "2bb9b119",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labelled = val.merge(val_labels_df, on='Trajectory', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "06b3868f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PARQUET = os.path.join(\"..\", \"..\", \"data\", \"aisdk\", \"clustered_train\", \"aisdk_2025\")\n",
    "VAL_PARQUET = os.path.join(\"..\", \"..\", \"data\", \"aisdk\", \"clustered_val\", \"aisdk_2025\")\n",
    "\n",
    "def save_parquet_partitioned(df: pd.DataFrame,\n",
    "                             out_path: str,\n",
    "                             partition_cols: list[str] = [\"MMSI\", \"Segment\"]) -> None:\n",
    "    \"\"\"Save DataFrame as a partitioned Parquet dataset.\"\"\"\n",
    "    print(f\"Saving to parquet dataset at {out_path} ...\")\n",
    "    table = pyarrow.Table.from_pandas(df, preserve_index=False)\n",
    "    pyarrow.parquet.write_to_dataset(\n",
    "        table,\n",
    "        root_path=out_path,\n",
    "        partition_cols=partition_cols,\n",
    "    )\n",
    "    print(\"Parquet save done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "d78a6815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to parquet dataset at ../../data/aisdk/clustered_train/aisdk_2025 ...\n",
      "Parquet save done.\n",
      "Saving to parquet dataset at ../../data/aisdk/clustered_val/aisdk_2025 ...\n",
      "Parquet save done.\n",
      "Parquet save done.\n",
      "Saving to parquet dataset at ../../data/aisdk/clustered_val/aisdk_2025 ...\n",
      "Parquet save done.\n"
     ]
    }
   ],
   "source": [
    "save_parquet_partitioned(train_labelled, out_path=TRAIN_PARQUET, partition_cols=[\"MMSI\", \"Trajectory\"])\n",
    "save_parquet_partitioned(val_labelled, out_path=VAL_PARQUET, partition_cols=[\"MMSI\", \"Trajectory\"])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
